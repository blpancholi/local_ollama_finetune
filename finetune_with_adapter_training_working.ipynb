{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4uz2ji82Eux"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # For better error messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cL3byIHfWEVx"
      },
      "outputs": [],
      "source": [
        "!pip install unsloth trl peft accelerate bitsandbytes pandas psutil psutils\n",
        "!pip install \"trl<0.15.0\" psutil --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOTWElUCWk_v"
      },
      "outputs": [],
      "source": [
        "# For GPU check\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x953lw83WxnY"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "model_name = \"unsloth/llama-3.2-3b-bnb-4bit\"\n",
        "\n",
        "max_seq_length = 2048  # Choose sequence length\n",
        "dtype = None  # Auto detection\n",
        "\n",
        "# Load model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=model_name,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nw-CYUceHoAn"
      },
      "outputs": [],
      "source": [
        "# !pip install \"trl<0.15.0\" psutil --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCg-dd8Byr_w"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# tokenizer = get_chat_template(\n",
        "#     tokenizer,\n",
        "#     chat_template = \"llama-3\", # Use Llama-3 instruction format\n",
        "# )\n",
        "\n",
        "# 2. Add LoRA Adapters\n",
        "# Add LoRA adapters\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=64,  # LoRA rank - higher = more capacity, more memory\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=128,  # LoRA scaling factor (usually 2x rank)\n",
        "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
        "    bias=\"none\",     # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Unsloth's optimized version\n",
        "    random_state=3407,\n",
        "    use_rslora=False,  # Rank stabilized LoRA\n",
        "    loftq_config=None, # LoftQ\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v08de3wAXdu6"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "import pandas as pd\n",
        "import json\n",
        "from datasets import Dataset\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "# ============================================\n",
        "# 1. LOAD JSON DATA (Updated for JSON format)\n",
        "# ============================================\n",
        "# Load your JSON file\n",
        "# Expected format:\n",
        "# [{\"instruction\": \"...\", \"input\": \"...\", \"output\": \"...\"}, ...]\n",
        "# OR\n",
        "# [{\"question\": \"...\", \"answer\": \"...\"}, ...]\n",
        "\n",
        "json_file_path = \"solar_qa.json\"  # Change this to your JSON file path\n",
        "\n",
        "with open(json_file_path, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(data)} samples from JSON\")\n",
        "print(f\"Sample: {data[0]}\")\n",
        "\n",
        "# ============================================\n",
        "# 2. CONVERT JSON TO DATASET FORMAT\n",
        "# ============================================\n",
        "\n",
        "# Define prompt template for your instruction/question/answer format\n",
        "prompt_template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{input_text}\n",
        "\n",
        "### Response:\n",
        "{response}{eos_token}\"\"\"\n",
        "\n",
        "# Alternative simple template (uncomment if preferred):\n",
        "# prompt_template = \"\"\"Question: {question}\n",
        "# Answer: {response}{eos_token}\"\"\"\n",
        "\n",
        "def convert_json_to_dataset(data):\n",
        "    \"\"\"Convert JSON data with various formats to dataset format\"\"\"\n",
        "    texts = []\n",
        "\n",
        "    for item in data:\n",
        "        # Handle different JSON formats\n",
        "        if \"instruction\" in item and \"input\" in item and \"output\" in item:\n",
        "            # Standard instruction-input-output format\n",
        "            instruction = item[\"instruction\"]\n",
        "            input_text = item[\"input\"]\n",
        "            output = item[\"output\"]\n",
        "\n",
        "        elif \"instruction\" in item and \"output\" in item:\n",
        "            # Instruction-output format (no input)\n",
        "            instruction = item[\"instruction\"]\n",
        "            input_text = \"\"\n",
        "            output = item[\"output\"]\n",
        "\n",
        "        elif \"question\" in item and \"answer\" in item:\n",
        "            # Question-answer format\n",
        "            instruction = item.get(\"instruction\", \"Please answer the following question\")\n",
        "            input_text = item[\"question\"]\n",
        "            output = item[\"answer\"]\n",
        "\n",
        "        else:\n",
        "            print(f\"Warning: Unknown format for item: {item}\")\n",
        "            continue\n",
        "\n",
        "        # Format the prompt\n",
        "        prompt = prompt_template.format(\n",
        "            instruction=instruction,\n",
        "            input_text=input_text,\n",
        "            response=output,\n",
        "            eos_token=tokenizer.eos_token\n",
        "        )\n",
        "\n",
        "        texts.append(prompt)\n",
        "\n",
        "    return texts\n",
        "\n",
        "# Convert data to texts\n",
        "texts = convert_json_to_dataset(data)\n",
        "\n",
        "# Create dataset\n",
        "dataset_dict = {\"text\": texts}\n",
        "dataset = Dataset.from_dict(dataset_dict)\n",
        "\n",
        "print(f\"Dataset created with {len(dataset)} samples\")\n",
        "print(f\"\\nSample prompt:\\n{texts[0][:500]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lm8booC8XliQ"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "import psutil\n",
        "\n",
        "\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 1, # Changed from 2 to 1 to fix psutil issues\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 10,\n",
        "        # max_steps = 60, # Set a limit for quick testing (e.g., 1 epoch) #commented for finetuninng\n",
        "        num_train_epochs = 50, # added for more fine tune training\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\", # \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",\n",
        "    ),\n",
        "  \n",
        ")\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "# model = model.merge_and_unload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krG4bOTA1qQF"
      },
      "outputs": [],
      "source": [
        "# Commented to check resposne before merging , it may require back to merge the adapter to model\n",
        "# import torch\n",
        "# import gc\n",
        "\n",
        "# print(\"Step 1: Merging LoRA adapters...\")\n",
        "\n",
        "# # Clear cache\n",
        "# if torch.cuda.is_available():\n",
        "#     torch.cuda.empty_cache()\n",
        "# gc.collect()\n",
        "\n",
        "# # Merge adapters\n",
        "# model = model.merge_and_unload()\n",
        "\n",
        "# # Save merged model temporarily\n",
        "# print(\"Step 2: Saving merged model...\")\n",
        "# model.save_pretrained(\"merged_model_temp\")\n",
        "# tokenizer.save_pretrained(\"merged_model_temp\")\n",
        "\n",
        "# # Clear everything\n",
        "# del model\n",
        "# gc.collect()\n",
        "# if torch.cuda.is_available():\n",
        "#     torch.cuda.empty_cache()\n",
        "\n",
        "# # Reload the merged model fresh\n",
        "# print(\"Step 3: Reloading merged model...\")\n",
        "# from unsloth import FastLanguageModel\n",
        "\n",
        "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "#     model_name=\"merged_model_temp\",\n",
        "#     max_seq_length=2048,\n",
        "#     dtype=None,\n",
        "#     load_in_4bit=False,  # Load full precision for inference\n",
        "# )\n",
        "\n",
        "# Prepare for inference\n",
        "# FastLanguageModel.for_inference(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vBGnx7DXuN9"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# 4. TEST THE FINE-TUNED MODEL\n",
        "# ============================================\n",
        "\n",
        "# Enable inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# Define test cases based on your data\n",
        "test_cases = [\n",
        "    {\"instruction\": \"Explain the following content clearly and accurately.\", \"input\": \"What type of inverter is proposed for the system?\"},\n",
        "    {\"instruction\":\"Explain the following content clearly and accurately.\", \"input\": \"How is GST applied to the solar project cost?\"},\n",
        "    {\"instruction\": \"Explain the following content clearly and accurately.\", \"input\": \"What warranty coverage is provided for the solar inverter?\"},\n",
        "]\n",
        "\n",
        "print(\"--- Model Predictions ---\\n\")\n",
        "\n",
        "for test in test_cases:\n",
        "    instruction = test[\"instruction\"]\n",
        "    input_text = test[\"input\"]\n",
        "\n",
        "    # Prepare prompt (without response part)\n",
        "    prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{input_text}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(\n",
        "        [prompt],\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Generate response\n",
        "    # outputs = model.generate(\n",
        "    #     **inputs,\n",
        "    #     max_new_tokens=128,\n",
        "    #     use_cache=True,\n",
        "    #     temperature=0.7,\n",
        "    #     top_p=0.9\n",
        "    # )\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=128,\n",
        "        temperature=0.2,  # Very low = more deterministic (reduce hallucination)\n",
        "        top_p=0.9,\n",
        "        top_k=40,  # Add top_k for more control\n",
        "        do_sample=True,\n",
        "        repetition_penalty=1.1,  # Reduce repetition\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        no_repeat_ngram_size=3,  # Prevent 3-gram repetition\n",
        "    )\n",
        "    # Decode and print results\n",
        "    prediction = tokenizer.batch_decode(\n",
        "        outputs[:, inputs.input_ids.shape[1]:],\n",
        "        skip_special_tokens=True\n",
        "    )[0]\n",
        "\n",
        "    print(f\"Instruction: {instruction}\")\n",
        "    print(f\"Input: {input_text}\")\n",
        "    print(f\"Response: {prediction.strip()}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twEmkIrLZLtD"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained_gguf(\"gguf_model\", tokenizer, quantization_method=\"q4_k_m\") # for balanced quality , sixe will be 2gb\n",
        "# model.save_pretrained_gguf(\"gguf_model\", tokenizer, quantization_method=\"Q8_0\") # for better quality sixe will be 3gb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ous51sIkSscY"
      },
      "outputs": [],
      "source": [
        "# STEPS TO PROCESS \n",
        "# Downlaod the file   Llama-3.2-3B.Q4_K_M.gguf and copy in project root folder\n",
        "# mMake the follwing changes -\n",
        "\n",
        "in docker-compose file :\n",
        "      - ./models:/root/.ollama/models   # store models locally for reuse\n",
        "      - .:/workspace                    # mount root directory to access custom GGUF files\n",
        "    environment:\n",
        "\n",
        "create or add in modelFile -\n",
        "  FROM /workspace/Llama-3.2-3B.Q4_K_M.gguf\n",
        "\n",
        "  # Optional: Set model parameters\n",
        "  PARAMETER temperature 0.7\n",
        "  PARAMETER top_p 0.9\n",
        "  PARAMETER top_k 40\n",
        "\n",
        "\n",
        "cd /Users/blpancholi/innovatechs_work/dev/ollama_test && docker-compose down\n",
        "\n",
        "\n",
        "cd /Users/blpancholi/innovatechs_work/dev/ollama_test && docker-compose up -d\n",
        "\n",
        "# docker exec -it ollama ollama create llama3.2-3b -f /workspace/Modelfile\n",
        "\n",
        "docker exec ollama ollama create llama3.2-3b -f /workspace/Modelfile\n",
        "\n",
        "\n",
        "docker exec ollama ollama list\n",
        "\n",
        "\n",
        "\n",
        "APi call -\n",
        "\n",
        "{\n",
        "  \"model\": \"llama3.2-3b\",\n",
        "  \"messages\": [\n",
        "    {\"role\": \"user\", \"content\": \"Tell me about APN company?\"}\n",
        "  ]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSmERd43la0l"
      },
      "outputs": [],
      "source": [
        "\n",
        "# from google.colab import files\n",
        "# import os\n",
        "\n",
        "# gguf_files = [f for f in os.listdir(\"gguf_model\") if f.endswith(\".gguf\")]\n",
        "# if gguf_files:\n",
        "#     gguf_file = os.path.join(\"gguf_model\", gguf_files[0])\n",
        "#     print(f\"Downloading: {gguf_file}\")\n",
        "#     files.download(gguf_file)\n",
        "\n",
        "# import shutil\n",
        "# from google.colab import files\n",
        "\n",
        "# # Create a zip archive of the folder\n",
        "# shutil.make_archive(\"gguf_model\", \"zip\", \"gguf_model\")\n",
        "\n",
        "# # Download the zip\n",
        "# files.download(\"gguf_model.zip\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
